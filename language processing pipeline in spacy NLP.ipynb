{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Spacy Language Processing Pipelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Nlp blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "sentence = nlp(\"Apply the pipeline to some text. The text can span multiple sentences, and can contain arbitrary whitespace.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply\n",
      "the\n",
      "pipeline\n",
      "to\n",
      "some\n",
      "text\n",
      ".\n",
      "The\n",
      "text\n",
      "can\n",
      "span\n",
      "multiple\n",
      "sentences\n",
      ",\n",
      "and\n",
      "can\n",
      "contain\n",
      "arbitrary\n",
      "whitespace\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for words in sentence:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get above error because we have a blank pipeline as shown below. Pipeline is something that starts with a Tokenizer. You can see there is nothing there hence the blank pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp.pipe_names is empty array indicating no components in the pipeline. Pipeline is something that starts with a tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Trained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_p=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_p.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1fad93470a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1fad9347b80>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1fad59dbb50>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1fad872cc40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1fad8775d40>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1fad59dbae0>)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_p.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nl_p(\"Apply the pipeline to some text. The text can span multiple sentences, and can contain arbitrary whitespace.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply  |  None\n",
      "the  |  None\n",
      "pipeline  |  None\n",
      "to  |  None\n",
      "some  |  None\n",
      "text  |  None\n",
      ".  |  None\n",
      "The  |  None\n",
      "text  |  None\n",
      "can  |  None\n",
      "span  |  None\n",
      "multiple  |  None\n",
      "sentences  |  None\n",
      ",  |  None\n",
      "and  |  None\n",
      "can  |  None\n",
      "contain  |  None\n",
      "arbitrary  |  None\n",
      "whitespace  |  None\n",
      ".  |  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ola\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "for words in sentence:\n",
    "    print(words, ' | ', spacy.explain(words.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in sentence.ents:\n",
    "        print(words.text, \" | \", words.label_, \" | \", spacy.explain(words.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output to the code above is None and the code below didn't bring out any output because it is a blank pipeline and does not recognize the given pipe name attribute, while the code below will bring out the expected result because it is a trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply  |  verb\n",
      "the  |  determiner\n",
      "pipeline  |  noun\n",
      "to  |  adposition\n",
      "some  |  determiner\n",
      "text  |  noun\n",
      ".  |  punctuation\n",
      "The  |  determiner\n",
      "text  |  noun\n",
      "can  |  auxiliary\n",
      "span  |  verb\n",
      "multiple  |  adjective\n",
      "sentences  |  noun\n",
      ",  |  punctuation\n",
      "and  |  coordinating conjunction\n",
      "can  |  auxiliary\n",
      "contain  |  verb\n",
      "arbitrary  |  adjective\n",
      "whitespace  |  noun\n",
      ".  |  punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, ' | ', spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Bank  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "1200  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nl_p(\"I owe First Bank $1200\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I owe \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    First Bank\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1200\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to add a pipe name for example 'ner' from a trained pipeline to a blank pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('ner', source=nl_p)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'lemmatizer', 'parser', 'tagger']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('lemmatizer', source=nl_p)\n",
    "nlp.add_pipe('parser', source=nl_p)\n",
    "nlp.add_pipe('tagger', source=nl_p)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Get all the proper nouns from a given text in a list and also count how many of them using a trained pipeline.\n",
    "- **Proper Noun** means a noun that names a particular person, place, or thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_p = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Proper nouns are:  [Ravi, Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad]\n",
      "There are 8 Proper nouns in the text\n"
     ]
    }
   ],
   "source": [
    "e = nl_p(text)\n",
    "proper_nouns = []\n",
    "for result in e:\n",
    "    if result.pos_ == 'PROPN':\n",
    "        proper_nouns.append(result)\n",
    "\n",
    "print('The Proper nouns are: ', proper_nouns)\n",
    "print('There are {} Proper nouns in the text'.format(len(proper_nouns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Get all companies names from a given text and also the count of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company names:  [Tesla, Walmart, Amazon, Microsoft, Google, Infosys, Reliance, HDFC Bank, Hindustan Unilever, Bharti Airtel]\n",
      "Count:  10\n"
     ]
    }
   ],
   "source": [
    "text = ''' The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys,  Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "Company_names = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        Company_names.append(ent)\n",
    "    \n",
    "print('Company names: ', Company_names)\n",
    "print('Count: ', len(Company_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a20c2438da76d01abbbd80122561b502df63077b005b417cc5df835b7dbcba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
